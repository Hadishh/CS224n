## Assignments of the "Natural Language Processing with Deep Learning" course (CS 224n, Stanford University)

This repository contains my solutions to the assignments of the course 
of "Natural Language Processing with Deep Learning" (CS224n) taught by Christopher Manning.
at the UC Berkeley. I used the material from 
[Winter 2021](http://web.stanford.edu/class/cs224n).

- [x] Assignment 1 - Exploring word vectors
- [x] Assignment 2 - Word2Vec implementation
- [x] Assignment 3 - Dependency Parsing 
- [x] Assignment 4 - Neural Machine Translation
- [x] Assignment 5 - Self-Attention, Transformers, and Pretraining


### Assignment 1 - Exploring word vectors
[Assignment 1](http://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring_word_vectors.html) is about word vectors such as GloVe and similarities between words. 
[Written Answer](https://github.com/Hadishh/CS224n/blob/main/cs224n_a1/exploring_word_vectors.pdf)

### Assignment 2 - Word2Vec Implementation
[Assignment 2](http://web.stanford.edu/class/cs224n/assignments/a2.pdf) is an implemetation of Word2Vec algorithm by sgd. 
[Written Answer](https://github.com/Hadishh/CS224n/blob/main/cs224n_a2/A2_Written_Answers.pdf)
### Assignment 3 - Dependency Parsing
[Assignment 3](http://web.stanford.edu/class/cs224n/assignments/a3.pdf) is about dependency parsing. Implenting a dependency parser. 
[Written Answer](https://github.com/Hadishh/CS224n/blob/main/cs224n_a3/A3_Report.pdf)

### Assignment 4 - Neural Machine Translation
[Assignment 4](http://web.stanford.edu/class/cs224n/assignments/a4.pdf) is about 
training a NMT with Recurrent Neural Networks. You probably need GPU for training. I used google colab.
[Written Answer](https://github.com/Hadishh/CS224n/blob/main/cs224n_a4/A4_Answers.pdf)

### Assignment 5 - Self-Attention, Transformers, and Pretraining
[Assignment 5](http://web.stanford.edu/class/cs224n/assignments/a5.pdf) is about transformers and self-attention. 
[Written Answer](https://github.com/Hadishh/CS224n/blob/main/cs224n_a5/A5_Answers.pdf)